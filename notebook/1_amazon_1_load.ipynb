{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4721f7d-9c44-48ca-893f-dbe90fa6ded5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import collections\n",
    "from tqdm import tqdm\n",
    "HERE = %pwd\n",
    "sys.path.append(os.path.dirname(HERE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93a83782-d3fc-4d46-841e-990988ec33d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# token counter\n",
    "import tiktoken\n",
    "from tiktoken.core import Encoding\n",
    "encoding = tiktoken.get_encoding(\"o200k_base\")\n",
    "\n",
    "\n",
    "def compute_token(text):\n",
    "    return len(encoding.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6208b47-ab0e-4328-b07f-27ae1b2b2bdb",
   "metadata": {},
   "source": [
    "1. Download from https://nijianmo.github.io/amazon/index.html  \n",
    "1. Place each dataset into `dir_save` following the format `f\"{dir_save}/Music/CDs_and_Vinyl_5.json.gz\"`. Please refer to `dict_amazon_name` in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830d71a7-91fc-4586-8670-72dca2fda47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data directory that save raw data\n",
    "dir_save = \"../data/raw_data/amazon18\"\n",
    "\n",
    "# directry name (key) and raw data name (value)\n",
    "dict_amazon_name = {\n",
    "    \"Music\" : \"CDs_and_Vinyl\",\n",
    "    \"Movie\" : \"Movies_and_TV\",\n",
    "    \"Book\" : \"Books\",\n",
    "    \"Grocery\" : \"Grocery_and_Gourmet_Food\",\n",
    "    \"Clothes\" : \"Clothing_Shoes_and_Jewelry\",\n",
    "    \"Beauty\" : \"All_Beauty\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "886497a9-fe3e-4045-a82d-80caaa8edf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def amazon(data_name, dir_save):\n",
    "    amazon_name = dict_amazon_name[data_name]\n",
    "    dir_path = f\"{dir_save}/{data_name}\"\n",
    "    path_records = f\"{dir_path}/{amazon_name}_5.json.gz\"\n",
    "    path_items = f\"{dir_path}/meta_{amazon_name}.json.gz\"\n",
    "\n",
    "    def _records():\n",
    "        # load from transaction records\n",
    "        g = gzip.open(path_records, 'rb')\n",
    "        dict_ = {}\n",
    "        idx = 0\n",
    "        for l in tqdm(g):\n",
    "            d_ = json.loads(l)\n",
    "        \n",
    "            try:\n",
    "                user = d_['reviewerID']\n",
    "                item = d_['asin']\n",
    "                rating = d_['overall']\n",
    "                \n",
    "                time = d_[\"reviewTime\"]\n",
    "                t_, year = time.split(\", \")\n",
    "                time = f\"{year}{t_[0:2]}{t_[3:]:0>2}\"\n",
    "\n",
    "                try:\n",
    "                    text = d_[\"reviewText\"]\n",
    "                    # remove long reviews because they tend to be meaningless texts (e.g., html) \n",
    "                    if compute_token(text) > 300: \n",
    "                        text = \"\"\n",
    "                except:\n",
    "                    text = \"\"\n",
    "                \n",
    "                l = [item, rating, time, text]\n",
    "                if user in dict_.keys():\n",
    "                    dict_[user].append(l)\n",
    "                else:\n",
    "                    dict_[user] = [l]\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        # transform dict to pandas.DataFrame\n",
    "        def _reshape(user):\n",
    "            df = pd.DataFrame(dict_[user], columns=['itemID', 'rating', 'time', 'review'])\n",
    "            df.insert(0, \"userID\", user)\n",
    "            return df\n",
    "        \n",
    "        df_ = pd.concat([_reshape(user) for user in tqdm(dict_.keys())])\n",
    "        df_.reset_index(inplace=True, drop=True)\n",
    "\n",
    "        # sort chronological order\n",
    "        df_ = df_.sort_values(by=\"time\", ascending=False)  \n",
    "\n",
    "        # delete dupicated items\n",
    "        df_records = df_.drop_duplicates(subset=[\"userID\", \"itemID\"], keep='first')  \n",
    "        return df_records\n",
    "    \n",
    "    def _item():\n",
    "        # load from item master\n",
    "        g = gzip.open(path_items, 'rb')\n",
    "        dict_ = {}\n",
    "        idx = 0\n",
    "        for l in tqdm(g):\n",
    "            d_ = json.loads(l)\n",
    "            item = d_['asin']\n",
    "    \n",
    "            # title\n",
    "            try:\n",
    "                title = d_['title']\n",
    "            except:\n",
    "                title = \"\"\n",
    "    \n",
    "            ## if title was written in html, skip it\n",
    "            ## long word title tend to be wierd, so skip it\n",
    "            ## short word title tend to be wierd, so skip it\n",
    "            if (\"class=\" in title) or (compute_token(title) > 100) or (len(set(title)) <= 3):\n",
    "                title = \"\"\n",
    "            \n",
    "            # category\n",
    "            try:\n",
    "                cat = d_['category']\n",
    "                cat = [c.replace(\"&amp;\", \"&\") for c in cat[1:]]\n",
    "                cat = [c for c in cat if len(c) <= 50]\n",
    "                categories = \", \".join(cat)\n",
    "            except:\n",
    "                categories = \"\"\n",
    "            \n",
    "            # description\n",
    "            try:\n",
    "                description = \", \".join(d_['description'])\n",
    "\n",
    "                ## remove wierd text\n",
    "                if len(set(description)) <= 3:\n",
    "                    description = \"\"\n",
    "\n",
    "                ## delete html text\n",
    "                flag = [\"class=\", \"<br>\", \"<br />\", \"<I>\", \"href\", \"xml\"]\n",
    "                if np.sum([f in description for f in flag]) > 0:\n",
    "                    description = \"\"\n",
    "\n",
    "                ## delete items whose description were longer than 300 tokens\n",
    "                if compute_token(description) >= 300:\n",
    "                    description = \"\"\n",
    "            except:\n",
    "                description = \"\"\n",
    "\n",
    "            l = [title, categories, description]\n",
    "            if item in dict_.keys():\n",
    "                # update longer description\n",
    "                if len(description) > len(dict_[item][2]):\n",
    "                    dict_[item] = l\n",
    "            else:\n",
    "                dict_[item] = l\n",
    "                \n",
    "        df_ = pd.DataFrame(dict_, index=[\"title\", \"categories\", \"description\"]).T\n",
    "        df_ = df_.dropna()\n",
    "        df_items = df_[df_[\"title\"] != \"\"]        \n",
    "        return df_items\n",
    "    \n",
    "    df_records = _records()\n",
    "    df_items = _item()\n",
    "    return df_records, df_items\n",
    "\n",
    "\n",
    "def remove_un_used_items(df_records, df_items):\n",
    "    # items registered in item master\n",
    "    items_master = set(df_items.index.values)\n",
    "    \n",
    "    # restrict transaction records whose rows are registered in items_master.\n",
    "    s = df_records['itemID'].apply(lambda s : s in items_master)\n",
    "    df_r = df_records[s]\n",
    "\n",
    "    # restric item master whose items are registed in restricted transcation records\n",
    "    df_i = df_items.loc[df_r['itemID'].unique()]\n",
    "    return df_r, df_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903504c1-c41f-4108-ad1c-7e616cb3f51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10408381it [25:55, 6727.92it/s] "
     ]
    }
   ],
   "source": [
    "# directory to save preprocessed data to reduce dataset size\n",
    "version_input = \"20250403_input\"\n",
    "\n",
    "# datasets\n",
    "data_names = [\"Music\", \"Movie\", \"Grocery\", \"Clothes\", \"Book\"]\n",
    "for data_name in data_names:\n",
    "    # load\n",
    "    df_records, df_items = amazon(data_name, dir_save)\n",
    "    df_records_master, df_item_master = remove_un_used_items(df_records, df_items)\n",
    "\n",
    "    # groupby\n",
    "    gb_master = df_records_master.groupby(\"userID\")\n",
    "    users_master = df_records_master[\"userID\"].unique()\n",
    "    L = [gb_master.get_group(user) for user in users_master]\n",
    "\n",
    "    # user type : heavy or light\n",
    "    du = {\n",
    "        \"light\" : {\n",
    "            \"min\" : 5,\n",
    "            \"max\" : 10\n",
    "        },\n",
    "        \"heavy\" : {\n",
    "            \"min\" : 30,\n",
    "            \"max\" : 50\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    di = dict()\n",
    "    for type_user, dn in du.items():\n",
    "        # for evaluation, add another user\n",
    "        n_min = dn[\"min\"] + 1\n",
    "        n_max = dn[\"max\"] + 1\n",
    "\n",
    "        # select users whoose transactions satisfied the user type constraints\n",
    "        df_records_ = pd.concat([\n",
    "            df for df in L \n",
    "            if len(df) >= n_min\n",
    "            and len(df) <= n_max\n",
    "            and df[\"rating\"].iloc[-1] > 3 \n",
    "        ])\n",
    "        df_records, df_items = remove_un_used_items(df_records_, df_item_master)\n",
    "    \n",
    "        # groupby\n",
    "        gb = df_records.groupby(\"userID\")\n",
    "        users = df_records[\"userID\"].unique()\n",
    "        \n",
    "        # select users\n",
    "        ## +5 is for supplementary\n",
    "        users_sample = np.random.choice(users, size=200+5, replace=False)\n",
    "    \n",
    "        # create transaction records for the selected users\n",
    "        df_r = pd.concat([\n",
    "            gb.get_group(user).sort_values(by=\"time\", ascending=True)\n",
    "            for user in users_sample\n",
    "        ])\n",
    "    \n",
    "        # select candidate items \n",
    "        items_core = df_r[\"itemID\"].unique()\n",
    "        items_all = df_item_master.index.values\n",
    "        items_others = sorted(set(items_all) - set(items_core))\n",
    "        items_others = np.random.choice(items_others, size=500, replace=False)\n",
    "        items_candi = sorted(set(items_core).union(set(items_others)))\n",
    "        di[type_user] = items_candi\n",
    "    \n",
    "        d_ = {\n",
    "            \"data\" : data_name,\n",
    "            \"user_type\" : type_user,\n",
    "            \"#user\" : len(users_sample),\n",
    "            \"#item_candi\" : len(items_candi),\n",
    "            \"#user_all\" : len(users),\n",
    "            \"#item_all\" : len(df_item_master)\n",
    "        }\n",
    "        print(d_)\n",
    "        \n",
    "        # save\n",
    "        dir_save_data = f\"../data/preprocessed_data/{version_input}/Amazon_{data_name}\"\n",
    "        os.makedirs(dir_save_data, exist_ok=True)\n",
    "        df_r.to_csv(f\"{dir_save_data}/records_{type_user}.csv\")\n",
    "\n",
    "    # items used in both heavy and light to reduce data size\n",
    "    items = sorted(set(np.concatenate(list(di.values()))))\n",
    "    df_i = df_item_master.loc[items]\n",
    "    df_i.to_csv(f\"{dir_save_data}/items.csv\")\n",
    "\n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f093412-cc15-4a44-8600-0683154bd007",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbbd42d-acf3-4a50-a0b7-abc447b41446",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
