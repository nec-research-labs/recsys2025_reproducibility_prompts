{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4721f7d-9c44-48ca-893f-dbe90fa6ded5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import collections\n",
    "from tqdm import tqdm\n",
    "HERE = %pwd\n",
    "sys.path.append(os.path.dirname(HERE))\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a1bda84-9643-4ed6-9d36-bc69444d7f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 205/205 [00:00<00:00, 208.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie light 2709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 205/205 [00:00<00:00, 226.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie heavy 6246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 205/205 [00:01<00:00, 192.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Music light 2820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 205/205 [00:01<00:00, 197.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Music heavy 7063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 205/205 [00:00<00:00, 230.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grocery light 2693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 205/205 [00:00<00:00, 232.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grocery heavy 5937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 205/205 [00:00<00:00, 212.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clothes light 2892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 205/205 [00:01<00:00, 204.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clothes heavy 6239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 205/205 [00:01<00:00, 175.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book light 2998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 205/205 [00:01<00:00, 175.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book heavy 7903\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "# preprocessed data\n",
    "version_input = \"20250403_input\"\n",
    "\n",
    "# directory to save preprocessed data to use experiments\n",
    "version_prep = \"20250403_prep\"\n",
    "\n",
    "data_names = [\"Movie\", \"Music\", \"Grocery\", \"Clothes\", \"Book\"]\n",
    "for data_name in data_names:\n",
    "    dir_load_data = f\"../data/preprocessed_data/{version_input}/Amazon_{data_name}\"\n",
    "    df_items = pd.read_csv(f\"{dir_load_data}/items.csv\", index_col=0).fillna(\"\")\n",
    "    items_all = set(df_items.index.values)\n",
    "    \n",
    "    dir_save_data = f\"../data/preprocessed_data/{version_prep}/Amazon_{data_name}\"\n",
    "    os.makedirs(dir_save_data, exist_ok=True)\n",
    "    \n",
    "    di = dict()\n",
    "    for type_user in [\"light\", \"heavy\"]:\n",
    "        # load\n",
    "        df_records = pd.read_csv(f\"{dir_load_data}/records_{type_user}.csv\", index_col=0).fillna(\"\")\n",
    "        gb = df_records.groupby(\"userID\")\n",
    "        users = df_records[\"userID\"].unique()\n",
    "        \n",
    "        def _extract(user):\n",
    "            df_ = gb.get_group(user)\n",
    "            df_j = df_.join(df_items, on=\"itemID\")\n",
    "            # for in-context learning\n",
    "            items_train = sorted(set(df_j[\"itemID\"].unique()))\n",
    "\n",
    "            # for evaluation; 1 test + 9 others = 10 candidates \n",
    "            items_test = df_j[\"itemID\"].iloc[-1:].values\n",
    "            items_others = sorted(items_all - set(items_train))\n",
    "            items_others = np.random.choice(items_others, size=9)\n",
    "            items_candi = sorted(set(items_others).union(items_test))\n",
    "\n",
    "            d = {\n",
    "                \"id_train\" : items_train, \n",
    "                \"id_candi\" : items_candi\n",
    "            }\n",
    "            return d\n",
    "        \n",
    "        d_ = {user : _extract(user) for user in tqdm(users)}\n",
    "\n",
    "        # items that appeared for experiments\n",
    "        items_train = np.unique(np.concatenate([d[\"id_train\"] for d in d_.values()]))\n",
    "        items_candi = np.unique(np.concatenate([d[\"id_candi\"] for d in d_.values()]))\n",
    "        items = sorted(set(items_train).union(set(items_candi)))\n",
    "        di[type_user] = items\n",
    "    \n",
    "        # save\n",
    "        import pickle\n",
    "        with open(f\"{dir_save_data}/ids_{type_user}.pickle\", 'wb') as f:\n",
    "            pickle.dump(d_, f)\n",
    "        print(data_name, type_user, len(items))\n",
    "    \n",
    "    items = sorted(set(np.concatenate(list(di.values()))))\n",
    "    df_items_valid = df_items.loc[items]\n",
    "    df_items_valid.to_csv(f\"{dir_save_data}/items_slim.csv\")\n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e211f158-829a-41af-a93f-52c0196e7c69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
